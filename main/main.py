# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xbathWl8GgVK9KQxCpwyVy7ccsF7bWXi
"""

#from google.colab import drive
#drive.mount('/content/drive')


dr="/spoken_written/saved_weights/"

import pandas as pd
import pickle
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

para="many people told me that having son is better than having daughter . when i was five years old playing with three of my friends we found fifty five hundred dollar note and thoughts to share among us . but unfortunatel it was taken by sixty year old men who lives in u.s and he works in n a s a."


words=para.split()
seq_id=[len(words)+(i*0.1) for i in range(len(words))]

data=pd.DataFrame({'words':words,'sent_id':seq_id})

# loading
with open(dr+'label_spoken_tokenizer.pickle', 'rb') as handle:
    label_tokenizer = pickle.load(handle)

# encode and pad sequences
def encode_sequences(tokenizer, length, lines,seq_info):
    # integer encode sequences
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    seq=seq.astype(float)
    seq.T[-1]=seq_info#adding sequence and token information
    return seq

testX = encode_sequences(label_tokenizer, 5, data.iloc[:,0],data.iloc[:,1].tolist())

label_model=load_model(dr+'lstm_label_model.h5')

ar=label_model.predict(testX)

labels=['PLAIN_PUNC','NUMERIC','LETTERS_VERB', 'ELECTRONIC' ]

db=[ar[e].argmax() for e in range(len(ar))]
data['new_class']=db
data['dup']=data['words'].apply(lambda w: 2 if len(w)==1 else(2 if len(w.split('.'))>1 and len(w.split('.'))<5 else 0)) 
data['new_class']=data[["new_class", "dup"]].max(axis=1)

n=[]
for i,w in enumerate(data['new_class']):
  if(w==0):
    c=0
    q=0
    n.append(i)
  elif(w==1):
    if(c+1==i):
      n.append(k)
      c=i
    else:
      n.append(i)
      c=i
      k=i
  elif(w==2):
    if(q+1==i):
      n.append(p)
      q=i
    else:
      n.append(i)
      q=i
      p=i
      pass
    pass
  else:
    n.append(i)
data['dup']=n

df=data.groupby(['dup','new_class'])['words'].apply(lambda x: ' '.join(x.astype(str))).reset_index(level =0)
df=df.reset_index()

#class
num_df=df[df['new_class']==1]
let_df=df[df['new_class']==2]

#load tokenizers
# loading
with open(dr+'letter_spoken_tokenizer.pickle', 'rb') as handle:
    letter_spoken_tokenizer = pickle.load(handle)

with open(dr+'letter_written_tokenizer.pickle', 'rb') as handle:
    letter_written_tokenizer = pickle.load(handle)

with open(dr+'number_spoken_tokenizer.pickle', 'rb') as handle:
    number_spoken_tokenizer = pickle.load(handle)
    
with open(dr+'number_written_tokenizer.pickle', 'rb') as handle:
    number_written_tokenizer = pickle.load(handle)

num_model=load_model(dr+'num_model.h5')
letter_model=load_model(dr+'letter_model.h5')

#x_num=number_spoken_tokenizer.texts_to_sequences(num_df['words'])
def encode(tokenizer, length, lines):
    # integer encode sequences
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

def get_word(n, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == n:
            return word
    return None

def vect_to_text(preds,tokenizer):
  preds_text = []
  for i in preds:
      temp = []
      for j in range(len(i)):
          t = get_word(i[j], tokenizer)
          if j > 0:
              if (t == get_word(i[j-1], tokenizer)) or (t == None):
                  temp.append('')
              else:
                  temp.append(t)
          else:
              if(t == None):
                  temp.append('')
              else:
                  temp.append(t) 

      preds_text.append(' '.join(temp))
  return preds_text

#For numerical predicts
num_x = encode(number_spoken_tokenizer, 20,num_df['words'])
preds = num_model.predict_classes(num_x.reshape((num_x.shape[0],num_x.shape[1])))

#leter predicts
let_x = encode(letter_spoken_tokenizer, 20,let_df['words'])
num_preds = num_model.predict_classes(let_x.reshape((let_x.shape[0],let_x.shape[1])))

nums=vect_to_text(preds,number_written_tokenizer)
num_df['preds']=nums

letters=vect_to_text(num_preds,letter_written_tokenizer)
let_df['preds']=letters
let_df['preds']=let_df['words'].apply(lambda w:(w.replace(" ", "") ).upper())
let_df['preds']=let_df['preds'].str.strip()#remove spaces

final_data=data[data['new_class']==0]
final_data['preds']=final_data['words']
final_data['preds']=final_data['preds'].str.strip()#remove spaces
final_data.drop(['sent_id'],axis=1,inplace=True)

frames = [final_data, let_df, num_df]

result = pd.concat(frames)
result=result.sort_values(by=['dup'])
result['preds']=result['preds'].str.strip()

paragraph=' '.join(result['preds'].values)

