{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Lstm_number.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "39IZhhi0CIHF",
        "colab_type": "code",
        "outputId": "57a9c81c-cad4-4ab8-b990-bc67c82c6f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOdxfKqRpe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dr=\"/content/drive/My Drive/Classroom/Datasets/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ysezk7LTBUI",
        "colab_type": "code",
        "outputId": "adea1da7-3d2a-4590-c231-bc12fae2a621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "import os\n",
        "os.listdir(dr)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glove.6B',\n",
              " 'donar_preprocess.csv',\n",
              " 'donar_tfselected_words.csv',\n",
              " 'chunk3_train_sparse_matrix.npz',\n",
              " 'chunk2_train_sparse_matrix.npz',\n",
              " 'chunk1_train_sparse_matrix.npz',\n",
              " 'sample_test_sparse_matrix.npz',\n",
              " 'Donesaving_reg1.csv',\n",
              " 'reg_test.csv',\n",
              " 'reg_train1.csv',\n",
              " 'Donesaving_reg2.csv',\n",
              " 'reg_train2.csv',\n",
              " 'reg_train3.csv',\n",
              " 'random_file_size.csv',\n",
              " 'srandom_parse_matrix.npz',\n",
              " 'top_feat_names.npy',\n",
              " 'top_feat_indexs.npy',\n",
              " 'file_name_list.npy',\n",
              " 'file_size.csv',\n",
              " 'samplebyte_images.rar',\n",
              " 'samplebyte_images',\n",
              " 'trainLabels.csv',\n",
              " 'result.csv',\n",
              " 'asmoutputfile.csv',\n",
              " 'asm_result_with_size.csv',\n",
              " 'one_data.csv',\n",
              " 'final_byte.csv',\n",
              " 'cnn_feat.csv',\n",
              " 'IMDB_files_csv',\n",
              " 'en_train.csv',\n",
              " 'model_INT',\n",
              " 'label_model.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvcqG6ijHEny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5051e9c2-3742-46cb-b956-001e75498fdf"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23NTeqesHEn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyiQ_tnmHEn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(dr+\"en_train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnHjMLQMHEn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d63606c5-0723-4263-a03d-61933332f6c4"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9918441, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uz5iZUxHEoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=df[df['sentence_id']<=150000]#taking 1.5 million sentence data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SH5gScCNHEoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b892a94-307d-478d-e035-3acd9cbc8f5a"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1922423, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHCwPDlwHEoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "b707db21-e8e5-4045-8f8e-aea86878430f"
      },
      "source": [
        "data['class'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PLAIN         1418821\n",
              "PUNCT          367436\n",
              "DATE            54281\n",
              "LETTERS         28991\n",
              "CARDINAL        25820\n",
              "VERBATIM        15076\n",
              "MEASURE          3092\n",
              "ORDINAL          2385\n",
              "DECIMAL          2022\n",
              "MONEY            1224\n",
              "DIGIT            1011\n",
              "ELECTRONIC        957\n",
              "TELEPHONE         738\n",
              "TIME              245\n",
              "FRACTION          228\n",
              "ADDRESS            96\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBUpRnJbHEoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"new_class\"]=data['class'].apply(lambda c: \"NUMERIC\" if str(c)=='DATE' or  str(c)=='CARDINAL' or  str(c)=='MEASURE' or  str(c)=='ORDINAL' or  str(c)=='DECIMAL' or  str(c)=='MONEY' or  str(c)=='DIGIT' or  str(c)=='TELEPHONE' or str(c)=='TIME' or str(c)=='FRACTION' or  str(c)=='ADDRESS' else( \"PLAIN_PUNC\" if str(c)=='PLAIN' or str(c)=='PUNCT' else( \"LETTERS_VERB\" if str(c)=='LETTERS' or str(c)=='VERBATIM' else c ) ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcEx9traHEod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letter_df=data[data['new_class']=='LETTERS_VERB']\n",
        "df=data[data['new_class']=='NUMERIC']\n",
        "df.columns=['sentence_id',\t'token_id',\t'class',\t'written',\t'spoken',\t'new_class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIDf8MDrHEol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3kJ3LRKHEom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "36bc19e4-f8bb-4cb2-fae0-d3053c0c1707"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>class</th>\n",
              "      <th>written</th>\n",
              "      <th>spoken</th>\n",
              "      <th>new_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>DATE</td>\n",
              "      <td>2006</td>\n",
              "      <td>two thousand six</td>\n",
              "      <td>NUMERIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>DATE</td>\n",
              "      <td>2007</td>\n",
              "      <td>two thousand seven</td>\n",
              "      <td>NUMERIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>DATE</td>\n",
              "      <td>2008</td>\n",
              "      <td>two thousand eight</td>\n",
              "      <td>NUMERIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>CARDINAL</td>\n",
              "      <td>91</td>\n",
              "      <td>ninety one</td>\n",
              "      <td>NUMERIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>DATE</td>\n",
              "      <td>4 March 2014</td>\n",
              "      <td>the fourth of march twenty fourteen</td>\n",
              "      <td>NUMERIC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_id  token_id  ...                               spoken new_class\n",
              "10             1         0  ...                     two thousand six   NUMERIC\n",
              "51             3         7  ...                   two thousand seven   NUMERIC\n",
              "80             5         0  ...                   two thousand eight   NUMERIC\n",
              "95             6         3  ...                           ninety one   NUMERIC\n",
              "111            8         1  ...  the fourth of march twenty fourteen   NUMERIC\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eckIHDXGHEop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['spoken','written']]=df[['spoken','written']].astype('str')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flkLgdgSHEot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2171116-5662-45bb-da6e-ceb8054cdcdb"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(df['spoken'])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 20\n",
        "print('Size of English Vocabulary : %d' % eng_vocab_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of English Vocabulary : 326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lev1KzYHEov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c960c1fd-aad6-4bee-8b08-0f6768b0252a"
      },
      "source": [
        "# prepare Hindi tokenizer\n",
        "hin_tokenizer = tokenization(df['written'])\n",
        "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
        "\n",
        "hin_length = 20\n",
        "print('Size of Hindi Vocabulary : %d' % hin_vocab_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Hindi Vocabulary : 4267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWT6DHWaHEoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCdQwaF2HEo0",
        "colab_type": "text"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uETHIcqIgRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[['spoken','written']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Um461JkHEo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "25ba33ac-f1ba-48f2-c10a-a632b921a81b"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spoken</th>\n",
              "      <th>written</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>two thousand six</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>two thousand seven</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>two thousand eight</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>ninety one</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>the fourth of march twenty fourteen</td>\n",
              "      <td>4 March 2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  spoken       written\n",
              "10                      two thousand six          2006\n",
              "51                    two thousand seven          2007\n",
              "80                    two thousand eight          2008\n",
              "95                            ninety one            91\n",
              "111  the fourth of march twenty fourteen  4 March 2014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6UeiveTHEo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SPLITTING DATA\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.25, random_state = 42)\n",
        "# prepare training data\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train.iloc[:,0])\n",
        "trainY = encode_sequences(hin_tokenizer, hin_length, train.iloc[:,1])\n",
        "# prepare validation data\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test.iloc[:,0])\n",
        "testY = encode_sequences(hin_tokenizer, hin_length, test.iloc[:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxehTeC9HEo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f40ef5d3-c5f9-4c84-a999-7206dc5b0504"
      },
      "source": [
        "trainX.shape,trainY.shape,eng_length,hin_length,eng_vocab_size,hin_vocab_size"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((68356, 20), (68356, 20), 20, 20, 326, 4267)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2OAKI1uHEo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "802487d5-aa74-4dba-df19-195a42b3c68a"
      },
      "source": [
        "#model\n",
        "#ref:https://keras.io/layers/core/\n",
        "#ref:https://datascience.stackexchange.com/questions/46491/what-is-the-job-of-repeatvector-and-timedistributed\n",
        "model = Sequential()\n",
        "model.add(Embedding(eng_vocab_size, 512, input_length=eng_length, mask_zero=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(RepeatVector(hin_length))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dense(hin_vocab_size, activation='softmax'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Sp_bFDHEpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZyQE7L_HEpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38ec5344-46c1-4d28-c457-62ef664f95d4"
      },
      "source": [
        "#https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa#saving model\n",
        "filename = dr+'num_model.h5' \n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1 ), \n",
        "          epochs=10, batch_size=32, \n",
        "          validation_split = 0.2,\n",
        "          callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54684 samples, validate on 13672 samples\n",
            "Epoch 1/10\n",
            "54684/54684 [==============================] - 2273s 42ms/step - loss: 0.1193 - acc: 0.9810 - val_loss: 0.0991 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.09906, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 2/10\n",
            "54684/54684 [==============================] - 2292s 42ms/step - loss: 0.0889 - acc: 0.9866 - val_loss: 0.0897 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.09906 to 0.08970, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 3/10\n",
            "54684/54684 [==============================] - 2111s 39ms/step - loss: 0.0793 - acc: 0.9879 - val_loss: 0.0853 - val_acc: 0.9872\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.08970 to 0.08527, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 4/10\n",
            "54684/54684 [==============================] - 2107s 39ms/step - loss: 0.0737 - acc: 0.9886 - val_loss: 0.0823 - val_acc: 0.9876\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.08527 to 0.08229, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 5/10\n",
            "54684/54684 [==============================] - 2092s 38ms/step - loss: 0.0688 - acc: 0.9893 - val_loss: 0.0801 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.08229 to 0.08006, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 6/10\n",
            "54684/54684 [==============================] - 2084s 38ms/step - loss: 0.0651 - acc: 0.9897 - val_loss: 0.0782 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.08006 to 0.07823, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 7/10\n",
            "54684/54684 [==============================] - 2083s 38ms/step - loss: 0.0607 - acc: 0.9902 - val_loss: 0.0763 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.07823 to 0.07635, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 8/10\n",
            "54684/54684 [==============================] - 2074s 38ms/step - loss: 0.0571 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9887\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.07635 to 0.07529, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 9/10\n",
            "54684/54684 [==============================] - 2069s 38ms/step - loss: 0.0535 - acc: 0.9908 - val_loss: 0.0744 - val_acc: 0.9887\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.07529 to 0.07441, saving model to /content/drive/My Drive/Classroom/Datasets/num_model.h5\n",
            "Epoch 10/10\n",
            "14976/54684 [=======>......................] - ETA: 23:19 - loss: 0.0478 - acc: 0.9915"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2a2570951427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks=[checkpoint], verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGlKUExUoRQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving spoken token\n",
        "with open(dr+'number_spoken_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1CSGRvvpwmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving written token\n",
        "with open(dr+'number_written_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(hin_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}